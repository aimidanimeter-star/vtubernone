<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <title>WebAR Local File</title>
    <style>
        body { margin: 0; overflow: hidden; background-color: #000; font-family: sans-serif; }
        #video { position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover; transform: scaleX(-1); z-index: 1; }
        #canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; z-index: 2; pointer-events: none; }
        
        /* ファイル選択UI */
        #ui-layer {
            position: absolute; top: 0; left: 0; width: 100%; height: 100%;
            z-index: 999; display: flex; flex-direction: column;
            align-items: center; justify-content: center;
            background: rgba(0,0,0,0.6);
        }
        .btn-container {
            background: white; padding: 20px; border-radius: 12px; text-align: center;
        }
        h2 { margin: 0 0 10px 0; font-size: 18px; }
        p { font-size: 12px; color: #666; }
        input[type="file"] { margin-top: 10px; }
        
        #loading-msg {
            color: #0f0; font-weight: bold; margin-top: 20px; display: none;
            text-shadow: 1px 1px 0 #000;
        }
    </style>
    <script type="importmap">
    {
        "imports": {
            "three": "https://cdn.jsdelivr.net/npm/three@0.169.0/build/three.module.js",
            "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.169.0/examples/jsm/",
            "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@3.0.0/lib/three-vrm.module.js"
        }
    </script>
</head>
<body>
    <div id="ui-layer">
        <div class="btn-container">
            <h2>VRMファイルを選択してください</h2>
            <p>iPhone内の .vrm ファイルを選べば<br>すぐに表示されます</p>
            <input type="file" id="file-input" accept=".vrm">
            <div id="loading-msg">モデル読込中...</div>
        </div>
    </div>

    <video id="video" playsinline muted autoplay></video>
    <canvas id="canvas"></canvas>

    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/kalidokit@1.1.5/dist/kalidokit.umd.js"></script>

    <script type="module">
        import * as THREE from 'three';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { VRMLoaderPlugin, VRMUtils } from '@pixiv/three-vrm';

        // 要素取得
        const uiLayer = document.getElementById('ui-layer');
        const fileInput = document.getElementById('file-input');
        const loadingMsg = document.getElementById('loading-msg');

        // 3Dシーン設定
        const renderer = new THREE.WebGLRenderer({ canvas: document.getElementById('canvas'), alpha: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        renderer.setPixelRatio(window.devicePixelRatio);
        renderer.outputColorSpace = THREE.SRGBColorSpace;

        const scene = new THREE.Scene();
        const camera = new THREE.PerspectiveCamera(35.0, window.innerWidth / window.innerHeight, 0.1, 20.0);
        camera.position.set(0, 1.4, 1.5);

        const light = new THREE.DirectionalLight(0xffffff, 1.5);
        light.position.set(0, 0, 1).normalize();
        scene.add(light);
        scene.add(new THREE.AmbientLight(0xffffff, 0.8));

        let currentVrm = null;

        // ファイルが選択された時の処理
        fileInput.addEventListener('change', (event) => {
            const file = event.target.files[0];
            if (!file) return;

            loadingMsg.style.display = 'block';
            
            // ローカルファイルのURLを生成
            const blobUrl = URL.createObjectURL(file);
            loadVRM(blobUrl);
        });

        function loadVRM(url) {
            const loader = new GLTFLoader();
            loader.register((parser) => new VRMLoaderPlugin(parser));

            loader.load(
                url,
                (gltf) => {
                    const vrm = gltf.userData.vrm;
                    VRMUtils.removeUnnecessaryVertices(gltf.scene);
                    VRMUtils.combineSkeletons(gltf.scene);
                    vrm.scene.rotation.y = Math.PI; 
                    
                    scene.add(vrm.scene);
                    currentVrm = vrm;
                    
                    // UIを隠してカメラ開始
                    uiLayer.style.display = 'none';
                    startCamera();
                },
                (progress) => console.log('Loading...'),
                (error) => {
                    console.error(error);
                    alert("読み込みエラー！ファイルが壊れているか、正しいVRMではありません。");
                    loadingMsg.style.display = 'none';
                }
            );
        }

        // アニメーションループ
        const clock = new THREE.Clock();
        function animate() {
            requestAnimationFrame(animate);
            if (currentVrm) currentVrm.update(clock.getDelta());
            renderer.render(scene, camera);
        }
        animate();

        // 顔認識設定
        const videoElement = document.getElementById('video');
        function onResults(results) {
            if (!currentVrm || !results.multiFaceLandmarks || results.multiFaceLandmarks.length === 0) return;

            const landmarks = results.multiFaceLandmarks[0];
            const solver = Kalidokit.Face.solve(landmarks, { runtime: "mediapipe", video: videoElement });

            if (solver) {
                const rigRotation = solver.head;
                const head = currentVrm.humanoid.getNormalizedBoneNode('head');
                if(head) head.rotation.set(-rigRotation.x, -rigRotation.y, -rigRotation.z);

                if (currentVrm.expressionManager) {
                    currentVrm.expressionManager.setValue('aa', solver.mouth.shape.A || 0);
                    currentVrm.expressionManager.setValue('blinkLeft', 1 - (solver.eye.l || 1));
                    currentVrm.expressionManager.setValue('blinkRight', 1 - (solver.eye.r || 1));
                }
            }
        }

        async function startCamera() {
            const faceMesh = new FaceMesh({locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`});
            faceMesh.setOptions({ maxNumFaces: 1, refineLandmarks: true, minDetectionConfidence: 0.5, minTrackingConfidence: 0.5 });
            faceMesh.onResults(onResults);

            const camera = new Camera(videoElement, {
                onFrame: async () => { await faceMesh.send({image: videoElement}); },
                width: 1280, height: 720
            });
            await camera.start();
        }
        
        window.addEventListener('resize', () => {
            renderer.setSize(window.innerWidth, window.innerHeight);
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
        });
    </script>
</body>
</html>
